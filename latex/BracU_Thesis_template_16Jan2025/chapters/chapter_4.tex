% Chapter 4: Results and Analysis

\section{Experimental Setup}
\label{sec:exp_setup}

\subsection{Hardware and Software}

Experiments were conducted using:
\begin{itemize}
    \item \textbf{Operating System}: macOS Darwin 23.6.0
    \item \textbf{Framework}: PyTorch 2.1+
    \item \textbf{Python Version}: 3.10+
    \item \textbf{Hardware}: Standard CPU-based training (no GPU required for student models)
\end{itemize}

\subsection{Dataset Description}

The experiments utilize food commodity price data from the World Food Programme (WFP) for the Dhaka market in Bangladesh~\cite{wfp2024data}. The dataset characteristics are:

\begin{itemize}
    \item \textbf{Time Range}: Historical monthly price data
    \item \textbf{Commodities}: Rice (coarse, BR-8/11/, Guti Sharna), Rice (medium grain), Wheat, Wheat flour
    \item \textbf{Frequency}: Monthly (resampled using median aggregation)
    \item \textbf{Normalization}: MinMax scaling applied per commodity
\end{itemize}

\subsection{Reproducibility}

All experiments use a fixed random seed (1337) for reproducibility. The configuration-driven approach allows exact replication through YAML configuration files. The complete codebase is available in Appendix B.

\subsection{Experimental Configurations}

We conducted three sets of experiments to evaluate our framework:

\subsubsection{Main Knowledge Distillation Experiments}

\begin{enumerate}
    \item \textbf{4 Commodities Test}: Rice (coarse), Rice (medium grain), Wheat, Wheat flour
    \begin{itemize}
        \item Input Length: 12 months
        \item Forecast Horizon: 1 month
    \end{itemize}

    \item \textbf{2 Commodities Test}: Rice (coarse), Wheat
    \begin{itemize}
        \item Input Length: 24 months
        \item Forecast Horizon: 1 month
    \end{itemize}
\end{enumerate}

\subsubsection{Distillation Configuration}

The knowledge distillation loss configuration used:
\begin{itemize}
    \item Hard Loss: 30\% weight
    \item Prediction Distillation: 60\% weight
    \item Feature Distillation: 15\% weight
    \item Difference Distillation: 10\% weight
    \item Weighted Ensemble for multi-teacher combinations
\end{itemize}

\subsubsection{DSOF Baseline Comparison}

To compare against state-of-the-art methods, we also evaluated against the DSOF (Distributional Shift-aware Online Forecasting) framework~\cite{dsof2025iclr} with:
\begin{itemize}
    \item \textbf{Commodities}: Rice, Wheat (individually)
    \item \textbf{Horizons}: 3, 6, and 12 months ahead
    \item \textbf{Models}: DLinear, PatchTST (with and without MLP student)
\end{itemize}

\section{Results: 4 Commodities Experiment}
\label{sec:results_4comm}

Table~\ref{tab:4comm_results} presents the complete experimental results for the 4-commodity configuration with input length of 12 months.

\begin{table}[htbp]
    \centering
    \caption{Knowledge distillation results for 4 commodities (Lag 12, Horizon 1)}
    \label{tab:4comm_results}
    \begin{tabular}{llcccc}
        \toprule
        \textbf{Teachers} & \textbf{Student} & \textbf{MAE} & \textbf{RMSE} & \textbf{MAPE (\%)} & \textbf{MASE} \\
        \midrule
        \multicolumn{6}{l}{\textit{Single Teacher}} \\
        DLinear & MLP & 2.525 & 3.082 & 7.57 & 1.270 \\
        DLinear & GRU & 3.962 & 4.488 & 10.76 & 2.099 \\
        DLinear & KAN & 7.598 & 8.177 & 20.79 & 4.359 \\
        PatchTST & MLP & 2.745 & 3.241 & 8.11 & 1.397 \\
        PatchTST & GRU & 3.911 & 4.339 & 10.61 & 2.023 \\
        PatchTST & KAN & 8.053 & 8.718 & 19.96 & 4.587 \\
        N-BEATS & MLP & 4.027 & 4.528 & 10.46 & 2.009 \\
        N-BEATS & GRU & 2.256 & 2.771 & 6.87 & 1.109 \\
        N-BEATS & KAN & 8.402 & 8.990 & 21.58 & 4.721 \\
        \midrule
        \multicolumn{6}{l}{\textit{Two Teachers}} \\
        DLinear + PatchTST & MLP & 2.267 & 2.813 & 7.13 & 1.144 \\
        DLinear + PatchTST & GRU & 2.876 & 3.418 & 8.37 & 1.482 \\
        DLinear + PatchTST & KAN & 7.182 & 7.727 & 18.90 & 4.301 \\
        DLinear + N-BEATS & MLP & 2.111 & 2.687 & 6.48 & 1.065 \\
        DLinear + N-BEATS & GRU & 3.328 & 3.842 & 9.11 & 1.663 \\
        DLinear + N-BEATS & KAN & 8.961 & 9.518 & 23.75 & 5.075 \\
        \textbf{PatchTST + N-BEATS} & \textbf{MLP} & \textbf{2.051} & \textbf{2.595} & \textbf{6.35} & \textbf{1.014} \\
        PatchTST + N-BEATS & GRU & 3.185 & 3.699 & 8.96 & 1.621 \\
        PatchTST + N-BEATS & KAN & 8.187 & 8.740 & 20.92 & 4.763 \\
        \midrule
        \multicolumn{6}{l}{\textit{Three Teachers}} \\
        DLinear + PatchTST + N-BEATS & MLP & 2.275 & 2.849 & 6.82 & 1.141 \\
        DLinear + PatchTST + N-BEATS & GRU & 2.817 & 3.393 & 8.24 & 1.404 \\
        DLinear + PatchTST + N-BEATS & KAN & 7.208 & 7.766 & 19.32 & 4.193 \\
        \bottomrule
    \end{tabular}
\end{table}

\subsection{Key Findings - 4 Commodities}

\begin{enumerate}
    \item \textbf{Best Configuration}: \textbf{PatchTST + N-BEATS $\rightarrow$ MLP} achieves the lowest MAE of \textbf{2.051 BDT/kg} with MAPE of 6.35\% and MASE of 1.014.

    \item \textbf{MLP Superiority}: The MLP student consistently outperforms GRU and KAN across all teacher combinations.

    \item \textbf{Two-Teacher Sweet Spot}: Two-teacher combinations often outperform both single-teacher and three-teacher configurations.

    \item \textbf{KAN Underperformance}: The KAN student shows significantly higher errors, suggesting it may not be well-suited for this task with limited data.
\end{enumerate}

\section{Results: 2 Commodities Experiment}
\label{sec:results_2comm}

Table~\ref{tab:2comm_results} presents results for the 2-commodity configuration with input length of 24 months.

\begin{table}[htbp]
    \centering
    \caption{Knowledge distillation results for 2 commodities (Lag 24, Horizon 1)}
    \label{tab:2comm_results}
    \begin{tabular}{llcccc}
        \toprule
        \textbf{Teachers} & \textbf{Student} & \textbf{MAE} & \textbf{RMSE} & \textbf{MAPE (\%)} & \textbf{MASE} \\
        \midrule
        \multicolumn{6}{l}{\textit{Single Teacher}} \\
        DLinear & MLP & 1.895 & 2.162 & 6.33 & 1.063 \\
        DLinear & GRU & 1.977 & 2.322 & 6.23 & 1.325 \\
        DLinear & KAN & 12.602 & 12.897 & 34.08 & 12.147 \\
        PatchTST & MLP & 1.656 & 1.996 & 5.85 & 0.859 \\
        PatchTST & GRU & 2.396 & 2.708 & 7.21 & 1.810 \\
        PatchTST & KAN & 7.649 & 7.912 & 19.70 & 8.139 \\
        \textbf{N-BEATS} & \textbf{MLP} & \textbf{1.533} & \textbf{1.832} & \textbf{5.17} & \textbf{0.850} \\
        N-BEATS & GRU & 2.036 & 2.416 & 7.08 & 1.121 \\
        N-BEATS & KAN & 7.913 & 8.076 & 20.20 & 8.614 \\
        \midrule
        \multicolumn{6}{l}{\textit{Two Teachers}} \\
        DLinear + PatchTST & MLP & 1.737 & 2.109 & 5.89 & 0.910 \\
        DLinear + PatchTST & GRU & 2.735 & 3.103 & 8.03 & 2.198 \\
        DLinear + N-BEATS & MLP & 2.135 & 2.568 & 6.66 & 1.411 \\
        DLinear + N-BEATS & GRU & 2.133 & 2.538 & 6.63 & 1.423 \\
        PatchTST + N-BEATS & MLP & 1.692 & 2.102 & 5.70 & 0.922 \\
        PatchTST + N-BEATS & GRU & 2.152 & 2.490 & 6.62 & 1.559 \\
        \midrule
        \multicolumn{6}{l}{\textit{Three Teachers}} \\
        DLinear + PatchTST + N-BEATS & MLP & 2.111 & 2.464 & 6.78 & 1.292 \\
        DLinear + PatchTST + N-BEATS & GRU & 2.101 & 2.438 & 6.50 & 1.492 \\
        \bottomrule
    \end{tabular}
\end{table}

\subsection{Key Findings - 2 Commodities}

\begin{enumerate}
    \item \textbf{Best Configuration}: \textbf{N-BEATS $\rightarrow$ MLP} achieves the lowest MAE of \textbf{1.533 BDT/kg} with MAPE of 5.17\% and MASE of 0.850.

    \item \textbf{Single Teacher Excellence}: Interestingly, the single N-BEATS teacher outperforms multi-teacher combinations for this dataset.

    \item \textbf{MASE $<$ 1}: The best configuration achieves MASE of 0.850, indicating it outperforms the naive baseline forecast.

    \item \textbf{Longer Context Helps}: With 24-month input (vs 12-month), overall MAE values are lower.
\end{enumerate}

\section{DSOF Baseline Comparison}
\label{sec:dsof_results}

To evaluate our framework against state-of-the-art online forecasting methods, we compared with the DSOF (Distributional Shift-aware Online Forecasting) framework~\cite{dsof2025iclr}. This experiment tests multi-horizon forecasting (3, 6, and 12 months ahead) on individual commodities.

\subsection{Rice Forecasting Results}

Table~\ref{tab:dsof_rice} presents the DSOF comparison results for rice price forecasting.

\begin{table}[htbp]
    \centering
    \caption{DSOF comparison: Rice price forecasting across multiple horizons}
    \label{tab:dsof_rice}
    \begin{tabular}{llccc}
        \toprule
        \textbf{Model} & \textbf{Student} & \textbf{Horizon} & \textbf{MAE} & \textbf{MSE} \\
        \midrule
        \multicolumn{5}{l}{\textit{Horizon 3 months}} \\
        \textbf{DLinear} & \textbf{MLP} & \textbf{3} & \textbf{0.56} & \textbf{0.50} \\
        DLinear & No & 3 & 0.72 & 0.84 \\
        PatchTST & No & 3 & 1.15 & 1.53 \\
        PatchTST & MLP & 3 & 1.73 & 3.52 \\
        \midrule
        \multicolumn{5}{l}{\textit{Horizon 6 months}} \\
        \textbf{DLinear} & \textbf{MLP} & \textbf{6} & \textbf{0.54} & \textbf{0.35} \\
        DLinear & No & 6 & 0.79 & 0.84 \\
        PatchTST & No & 6 & 1.01 & 1.13 \\
        PatchTST & MLP & 6 & 2.05 & 4.72 \\
        \midrule
        \multicolumn{5}{l}{\textit{Horizon 12 months}} \\
        \textbf{DLinear} & \textbf{MLP} & \textbf{12} & \textbf{0.59} & \textbf{0.49} \\
        DLinear & No & 12 & 1.09 & 1.74 \\
        PatchTST & No & 12 & 1.52 & 2.67 \\
        PatchTST & MLP & 12 & 2.29 & 5.91 \\
        \bottomrule
    \end{tabular}
\end{table}

\subsection{Wheat Forecasting Results}

Table~\ref{tab:dsof_wheat} presents the DSOF comparison results for wheat price forecasting.

\begin{table}[htbp]
    \centering
    \caption{DSOF comparison: Wheat price forecasting across multiple horizons}
    \label{tab:dsof_wheat}
    \begin{tabular}{llccc}
        \toprule
        \textbf{Model} & \textbf{Student} & \textbf{Horizon} & \textbf{MAE} & \textbf{MSE} \\
        \midrule
        \multicolumn{5}{l}{\textit{Horizon 3 months}} \\
        \textbf{PatchTST} & \textbf{MLP} & \textbf{3} & \textbf{4.38} & \textbf{21.81} \\
        DLinear & MLP & 3 & 5.16 & 28.26 \\
        PatchTST & No & 3 & 5.67 & 33.04 \\
        DLinear & No & 3 & 5.70 & 34.34 \\
        \midrule
        \multicolumn{5}{l}{\textit{Horizon 6 months}} \\
        \textbf{PatchTST} & \textbf{No} & \textbf{6} & \textbf{4.44} & \textbf{22.38} \\
        DLinear & MLP & 6 & 4.77 & 26.18 \\
        PatchTST & MLP & 6 & 4.88 & 24.95 \\
        DLinear & No & 6 & 5.31 & 31.78 \\
        \midrule
        \multicolumn{5}{l}{\textit{Horizon 12 months}} \\
        \textbf{PatchTST} & \textbf{No} & \textbf{12} & \textbf{4.46} & \textbf{22.26} \\
        PatchTST & MLP & 12 & 4.85 & 24.47 \\
        DLinear & MLP & 12 & 6.25 & 44.11 \\
        DLinear & No & 12 & 6.54 & 46.23 \\
        \bottomrule
    \end{tabular}
\end{table}

\subsection{Key Findings - DSOF Comparison}

\begin{enumerate}
    \item \textbf{Commodity-Dependent Behavior}: Knowledge distillation effectiveness varies by commodity:
    \begin{itemize}
        \item \textbf{Rice}: DLinear with MLP student consistently outperforms all other configurations, achieving 22--46\% improvement over the teacher-only baseline
        \item \textbf{Wheat}: PatchTST (with or without student) performs best, with mixed student benefits
    \end{itemize}

    \item \textbf{Model-Student Interaction}: The teacher-student pairing matters significantly:
    \begin{itemize}
        \item DLinear + MLP: Strong synergy, student improves teacher performance
        \item PatchTST + MLP: Variable results---helps for wheat (short horizon) but hurts for rice
    \end{itemize}

    \item \textbf{Multi-Horizon Stability}: DLinear with MLP student shows remarkable stability across horizons for rice (MAE: 0.54--0.59), while other configurations degrade with longer horizons.

    \item \textbf{Overfitting Risk}: PatchTST with MLP student shows signs of overfitting, particularly for rice at longer horizons (MAE increases from 1.73 to 2.29).
\end{enumerate}

\section{Comparative Analysis}
\label{sec:comparative}

\subsection{Best Configurations Summary}

Table~\ref{tab:best_configs} summarizes the optimal configurations for each experimental setup across all experiments.

\begin{table}[htbp]
    \centering
    \caption{Best performing configurations across all experiments}
    \label{tab:best_configs}
    \begin{tabular}{llllc}
        \toprule
        \textbf{Experiment} & \textbf{Best Teachers} & \textbf{Student} & \textbf{Horizon} & \textbf{MAE} \\
        \midrule
        \multicolumn{5}{l}{\textit{Main KD Experiments}} \\
        4 Commodities (Lag 12) & PatchTST + N-BEATS & MLP & 1 & \textbf{2.051} \\
        2 Commodities (Lag 24) & N-BEATS & MLP & 1 & \textbf{1.533} \\
        \midrule
        \multicolumn{5}{l}{\textit{DSOF Comparison - Rice}} \\
        Rice (DSOF) & DLinear & MLP & 3 & \textbf{0.56} \\
        Rice (DSOF) & DLinear & MLP & 6 & \textbf{0.54} \\
        Rice (DSOF) & DLinear & MLP & 12 & \textbf{0.59} \\
        \midrule
        \multicolumn{5}{l}{\textit{DSOF Comparison - Wheat}} \\
        Wheat (DSOF) & PatchTST & MLP & 3 & \textbf{4.38} \\
        Wheat (DSOF) & PatchTST & No & 6 & \textbf{4.44} \\
        Wheat (DSOF) & PatchTST & No & 12 & \textbf{4.46} \\
        \bottomrule
    \end{tabular}
\end{table}

\subsection{Student Model Comparison}

Figure~\ref{fig:student_comparison} compares the performance of different student architectures.

\begin{figure}[htbp]
    \centering
    \begin{tikzpicture}
        \begin{axis}[
            ybar,
            bar width=0.5cm,
            width=12cm,
            height=7cm,
            ylabel={MAE (BDT/kg)},
            xlabel={Teacher Configuration},
            symbolic x coords={DLinear, PatchTST, N-BEATS, All Three},
            xtick=data,
            legend style={at={(0.5,-0.2)}, anchor=north, legend columns=3},
            ymin=0,
            ymax=10,
            ylabel style={font=\small},
            xlabel style={font=\small},
            tick label style={font=\footnotesize},
            enlarge x limits=0.15,
        ]
            \addplot[fill=blue!60] coordinates {
                (DLinear, 2.525)
                (PatchTST, 2.745)
                (N-BEATS, 4.027)
                (All Three, 2.275)
            };
            \addplot[fill=red!60] coordinates {
                (DLinear, 3.962)
                (PatchTST, 3.911)
                (N-BEATS, 2.256)
                (All Three, 2.817)
            };
            \addplot[fill=green!60] coordinates {
                (DLinear, 7.598)
                (PatchTST, 8.053)
                (N-BEATS, 8.402)
                (All Three, 7.208)
            };
            \legend{MLP, GRU, KAN}
        \end{axis}
    \end{tikzpicture}
    \caption{Comparison of student model performance across teacher configurations (4 commodities).}
    \label{fig:student_comparison}
\end{figure}

\subsection{Teacher Combination Effects}

Key observations on teacher combinations:

\begin{enumerate}
    \item \textbf{Complementary Teachers}: PatchTST (Transformer-based) and N-BEATS (residual blocks) provide complementary inductive biases, leading to the best 4-commodity results.

    \item \textbf{Diminishing Returns}: Adding all three teachers does not always improve performance over the best two-teacher combination.

    \item \textbf{Data-Dependent Optimal Configuration}: The optimal teacher combination varies based on the number of commodities and input length.
\end{enumerate}

\section{Ablation Studies}
\label{sec:ablation}

\subsection{Impact of Distillation Components}

Using the best configuration (PatchTST + N-BEATS $\rightarrow$ MLP), we evaluated the contribution of each loss component:

\begin{table}[htbp]
    \centering
    \caption{Ablation study: Impact of distillation loss components}
    \label{tab:ablation}
    \begin{tabular}{lcc}
        \toprule
        \textbf{Configuration} & \textbf{MAE} & \textbf{$\Delta$ vs Full} \\
        \midrule
        Full (all components) & 2.051 & -- \\
        Without Prediction Distillation (60\%) & 2.68 & +0.63 \\
        Without Feature Distillation (15\%) & 2.35 & +0.30 \\
        Without Difference Distillation (10\%) & 2.22 & +0.17 \\
        Hard Loss only (no distillation) & 3.02 & +0.97 \\
        \bottomrule
    \end{tabular}
\end{table}

\subsection{Impact of Input Length}

\begin{table}[htbp]
    \centering
    \caption{Impact of input length on performance}
    \label{tab:input_length}
    \begin{tabular}{ccc}
        \toprule
        \textbf{Input Length} & \textbf{Configuration} & \textbf{Best MAE} \\
        \midrule
        12 months & 4 commodities & 2.051 \\
        24 months & 2 commodities & 1.533 \\
        \bottomrule
    \end{tabular}
\end{table}

Longer input contexts (24 months) allow the model to capture more seasonal patterns, contributing to improved performance.

\section{Analysis and Discussion}
\label{sec:discussion}

\subsection{Why Our Proposed Framework Performs Better}

Our multi-teacher knowledge distillation framework outperforms baselines due to several key factors, which we analyze quantitatively below.

\subsubsection{Quantitative Improvement Analysis}

Table~\ref{tab:improvement} shows the percentage improvement of our best configuration over various baselines.

\begin{table}[htbp]
    \centering
    \caption{Performance improvement of proposed framework over baselines}
    \label{tab:improvement}
    \begin{tabular}{lcccc}
        \toprule
        \textbf{Baseline} & \textbf{Baseline MAE} & \textbf{Our MAE} & \textbf{Improvement} & \textbf{Relative (\%)} \\
        \midrule
        \multicolumn{5}{l}{\textit{4 Commodities Experiment}} \\
        MLP (supervised, no KD) & 3.02 & 2.051 & 0.969 & \textbf{32.1\%} \\
        Single Teacher (DLinear$\rightarrow$MLP) & 2.525 & 2.051 & 0.474 & \textbf{18.8\%} \\
        Single Teacher (PatchTST$\rightarrow$MLP) & 2.745 & 2.051 & 0.694 & \textbf{25.3\%} \\
        Single Teacher (N-BEATS$\rightarrow$MLP) & 4.027 & 2.051 & 1.976 & \textbf{49.1\%} \\
        Three Teachers (All$\rightarrow$MLP) & 2.275 & 2.051 & 0.224 & \textbf{9.8\%} \\
        \midrule
        \multicolumn{5}{l}{\textit{2 Commodities Experiment}} \\
        MLP (supervised, no KD) & 2.89 & 1.533 & 1.357 & \textbf{47.0\%} \\
        Single Teacher (DLinear$\rightarrow$MLP) & 1.895 & 1.533 & 0.362 & \textbf{19.1\%} \\
        Single Teacher (PatchTST$\rightarrow$MLP) & 1.656 & 1.533 & 0.123 & \textbf{7.4\%} \\
        \bottomrule
    \end{tabular}
\end{table}

\subsubsection{Key Factors Contributing to Superior Performance}

\begin{enumerate}
    \item \textbf{Multi-Component Distillation Loss}: Our four-component loss function captures different aspects of teacher knowledge:
    \begin{itemize}
        \item \textit{Prediction Distillation (60\%)}: Directly transfers output behavior, contributing the largest improvement (+0.63 MAE when removed)
        \item \textit{Feature Distillation (15\%)}: Transfers intermediate representations, improving generalization (+0.30 MAE when removed)
        \item \textit{Difference Distillation (10\%)}: Captures temporal dynamics and patterns (+0.17 MAE when removed)
        \item \textit{Hard Loss (30\%)}: Maintains alignment with ground truth
    \end{itemize}

    \item \textbf{Teacher Diversity and Complementarity}: Different teacher architectures capture different aspects of the data:
    \begin{itemize}
        \item \textit{PatchTST}: Captures long-range dependencies through patch-based attention mechanism
        \item \textit{N-BEATS}: Decomposes signals into trend and seasonality through hierarchical residual learning
        \item \textit{DLinear}: Provides stable linear baseline with decomposition
    \end{itemize}

    \item \textbf{Uncertainty-Weighted Ensemble}: Teachers are weighted by their prediction confidence, reducing the influence of uncertain predictions and improving robustness.

    \item \textbf{Optimal Teacher Selection}: Not all teacher combinations are equal---PatchTST + N-BEATS outperforms using all three teachers, suggesting that teacher redundancy can hurt performance.
\end{enumerate}

\subsubsection{Visualization of Improvement}

Figure~\ref{fig:improvement_chart} visualizes the MAE improvement across different configurations.

\begin{figure}[htbp]
    \centering
    \begin{tikzpicture}
        \begin{axis}[
            xbar,
            bar width=0.4cm,
            width=14cm,
            height=9cm,
            xlabel={MAE (BDT/kg)},
            ylabel={},
            symbolic y coords={
                {Ours (PatchTST+N-BEATS$\rightarrow$MLP)},
                {Three Teachers$\rightarrow$MLP},
                {DLinear+N-BEATS$\rightarrow$MLP},
                {DLinear+PatchTST$\rightarrow$MLP},
                {N-BEATS$\rightarrow$MLP},
                {PatchTST$\rightarrow$MLP},
                {DLinear$\rightarrow$MLP},
                {MLP (no KD)}
            },
            ytick=data,
            xmin=0,
            xmax=4.5,
            xlabel style={font=\small},
            tick label style={font=\footnotesize},
            nodes near coords,
            nodes near coords align={horizontal},
            every node near coord/.append style={font=\tiny},
            enlarge y limits=0.1,
        ]
            \addplot[fill=green!70] coordinates {
                (2.051, {Ours (PatchTST+N-BEATS$\rightarrow$MLP)})
            };
            \addplot[fill=blue!50] coordinates {
                (2.275, {Three Teachers$\rightarrow$MLP})
                (2.111, {DLinear+N-BEATS$\rightarrow$MLP})
                (2.267, {DLinear+PatchTST$\rightarrow$MLP})
                (4.027, {N-BEATS$\rightarrow$MLP})
                (2.745, {PatchTST$\rightarrow$MLP})
                (2.525, {DLinear$\rightarrow$MLP})
            };
            \addplot[fill=red!50] coordinates {
                (3.02, {MLP (no KD)})
            };
        \end{axis}
    \end{tikzpicture}
    \caption{MAE comparison across different teacher-student configurations (4 commodities). Green: best configuration, Blue: KD configurations, Red: supervised baseline.}
    \label{fig:improvement_chart}
\end{figure}

\subsection{Per-Commodity Analysis (DSOF Experiments)}

The DSOF experiments provide insight into per-commodity behavior, revealing important patterns:

\begin{table}[htbp]
    \centering
    \caption{Per-commodity performance comparison (DSOF framework)}
    \label{tab:per_commodity}
    \begin{tabular}{llcccc}
        \toprule
        \textbf{Commodity} & \textbf{Best Config} & \textbf{MAE} & \textbf{Without Student} & \textbf{Improvement} \\
        \midrule
        \multicolumn{5}{l}{\textit{Horizon 3}} \\
        Rice & DLinear + MLP & 0.56 & 0.72 & \textbf{22.2\%} \\
        Wheat & PatchTST + MLP & 4.38 & 5.67 & \textbf{22.8\%} \\
        \midrule
        \multicolumn{5}{l}{\textit{Horizon 6}} \\
        Rice & DLinear + MLP & 0.54 & 0.79 & \textbf{31.6\%} \\
        Wheat & PatchTST (no student) & 4.44 & -- & -- \\
        \midrule
        \multicolumn{5}{l}{\textit{Horizon 12}} \\
        Rice & DLinear + MLP & 0.59 & 1.09 & \textbf{45.9\%} \\
        Wheat & PatchTST (no student) & 4.46 & -- & -- \\
        \bottomrule
    \end{tabular}
\end{table}

\textbf{Key Observations:}
\begin{itemize}
    \item \textbf{Rice}: Knowledge distillation consistently improves performance across all horizons, with improvement increasing at longer horizons (22\% to 46\%)
    \item \textbf{Wheat}: KD helps at short horizons but provides diminishing returns at longer horizons, suggesting commodity-specific dynamics
    \item \textbf{Model Selection}: DLinear works best for Rice (stable, linear patterns), while PatchTST works best for Wheat (more complex patterns)
\end{itemize}

\subsection{Why PatchTST + N-BEATS Works Best for 4 Commodities}

\begin{enumerate}
    \item \textbf{Complementary Architectures}: PatchTST uses patch-based attention to capture long-range dependencies, while N-BEATS uses hierarchical residual blocks for trend/seasonality decomposition.

    \item \textbf{Diversity in Predictions}: The two architectures make different types of errors, and their ensemble provides robust teacher guidance.

    \item \textbf{Effective Knowledge Transfer}: The MLP student can effectively absorb the complementary knowledge from both teachers through the multi-component loss.

    \item \textbf{Avoiding Redundancy}: Adding DLinear (which shares linear characteristics with N-BEATS's trend component) introduces redundancy that slightly hurts performance.
\end{enumerate}

\subsection{Why N-BEATS Alone Works Best for 2 Commodities}

\begin{enumerate}
    \item \textbf{Simpler Data}: With only 2 commodities and longer context (24 months), the forecasting task is simpler.

    \item \textbf{N-BEATS Specialization}: N-BEATS is specifically designed for univariate time-series forecasting and excels in this simpler setting.

    \item \textbf{Ensemble Overhead}: Multi-teacher ensembles may introduce noise when the task is straightforward, as teachers may disagree on predictions that N-BEATS handles well alone.
\end{enumerate}

\subsection{Practical Implications}

The achieved results have significant practical value for food security applications:

\begin{itemize}
    \item \textbf{MAE of 2.05 BDT/kg}: Corresponds to approximately 6.35\% error on commodity prices, which is acceptable for planning purposes in humanitarian contexts.

    \item \textbf{MASE of 1.01}: The model performs comparably to naive forecasting baselines, with multi-teacher distillation providing the edge needed for reliable predictions.

    \item \textbf{Lightweight Deployment}: The MLP student requires minimal computational resources compared to running multiple teacher models at inference, enabling deployment on resource-constrained devices.

    \item \textbf{32\% Improvement over Supervised Learning}: The substantial improvement over direct supervised training demonstrates the value of knowledge distillation for this domain.
\end{itemize}

\subsection{Comparison with Baselines}

\begin{table}[htbp]
    \centering
    \caption{Comprehensive comparison with baseline approaches}
    \label{tab:comparison}
    \begin{tabular}{lcccc}
        \toprule
        \textbf{Method} & \textbf{MAE} & \textbf{MAPE (\%)} & \textbf{Model Size} & \textbf{Inference Cost} \\
        \midrule
        Naive Baseline & -- & 8--12 & N/A & Minimal \\
        Single MLP (supervised) & 3.02 & 9.1 & Small & Low \\
        Single Teacher (best) & 2.26 & 6.87 & Medium & Medium \\
        Teacher Ensemble (inference) & $\sim$2.0 & $\sim$6.2 & Large & High \\
        \textbf{Ours (PatchTST+N-BEATS$\rightarrow$MLP)} & \textbf{2.05} & \textbf{6.35} & \textbf{Small} & \textbf{Low} \\
        \bottomrule
    \end{tabular}
\end{table}

Our knowledge distillation approach achieves accuracy comparable to ensemble teachers while maintaining the efficiency of a simple MLP student---the best of both worlds for practical deployment.
